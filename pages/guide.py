import streamlit as st

st.set_page_config(page_title="Model Guide", layout="wide", page_icon="ğŸ“š")

st.title("ğŸ“š AI Model Selection Guide")
st.write("Choose the right model for your needs")

# Introduction
st.markdown("""
This guide helps you understand the capabilities, strengths, and ideal use cases 
for each available AI model in our system.
""")

st.divider()

# Groq Models Section
st.header("ğŸš€ Groq Models (Ultra-Fast Inference)")

col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ¦™ llama-3.3-70b-versatile")
    
    st.markdown("""
    **Provider:** Groq  
    **Size:** 70 billion parameters  
    **Speed:** âš¡âš¡âš¡ Very Fast (2-5 seconds)
    
    ### âœ… Best For:
    - General conversational AI
    - Complex reasoning tasks
    - Multi-turn conversations
    - Creative writing
    - Code generation
    
    ### ğŸ’ª Strengths:
    - Balanced performance across all tasks
    - Excellent reasoning capabilities
    - Fast response times
    - Good at following instructions
    
    ### âš ï¸ Limitations:
    - May hallucinate on very recent events
    - Not specialized for any specific domain
    
    ### ğŸ¯ Recommended Use Cases:
    ```
    âœ“ "Explain quantum computing"
    âœ“ "Write a Python function for..."
    âœ“ "Help me plan a trip to Japan"
    âœ“ "Debug this code..."
    âœ“ Multi-Agent Sequential Mode
    ```
    
    ### ğŸ’¡ Pro Tip:
    This is the **best all-around model** for most tasks. Great default choice!
    """)

with col2:
    st.subheader("âš¡ groq/compound-mini")
    
    st.markdown("""
    **Provider:** Groq  
    **Size:** Smaller, optimized model  
    **Speed:** âš¡âš¡âš¡âš¡ Ultra Fast (1-2 seconds)
    
    ### âœ… Best For:
    - Simple questions
    - Quick responses
    - Basic conversations
    - When speed is critical
    
    ### ğŸ’ª Strengths:
    - Fastest response times
    - Low latency
    - Efficient token usage
    - Good for simple tasks
    
    ### âš ï¸ Limitations:
    - âŒ **No web search support**
    - Limited reasoning capabilities
    - Shorter context window
    - Less creative outputs
    
    ### ğŸ¯ Recommended Use Cases:
    ```
    âœ“ "What is 2+2?"
    âœ“ "Define photosynthesis"
    âœ“ "Translate 'hello' to Spanish"
    âœ“ Simple Q&A
    âœ— Complex research questions
    âœ— Multi-Agent modes
    ```
    
    ### ğŸ’¡ Pro Tip:
    Use this when you need **speed over depth**. Not recommended for multi-agent.
    """)

st.divider()

col3, col4 = st.columns(2)

with col3:
    st.subheader("ğŸŒ openai/gpt-oss-120b")
    
    st.markdown("""
    **Provider:** Groq (OpenAI-compatible)  
    **Size:** 120 billion parameters  
    **Speed:** âš¡âš¡ Fast (3-6 seconds)
    
    ### âœ… Best For:
    - Advanced reasoning
    - Complex analysis
    - Professional writing
    - Technical documentation
    
    ### ğŸ’ª Strengths:
    - Largest Groq model available
    - Superior reasoning capabilities
    - Excellent for technical content
    - High-quality outputs
    
    ### âš ï¸ Limitations:
    - Slightly slower than other Groq models
    - Higher token costs
    
    ### ğŸ¯ Recommended Use Cases:
    ```
    âœ“ "Analyze this business proposal"
    âœ“ "Write technical documentation"
    âœ“ "Explain complex algorithms"
    âœ“ Multi-Agent Debate Mode
    âœ“ Professional content creation
    ```
    
    ### ğŸ’¡ Pro Tip:
    Best choice for **professional or academic** work requiring deep analysis.
    """)

with col4:
    st.info("**ğŸ’¡ Groq Summary:**\n\n"
            "- **Speed Champions:** All Groq models are ultra-fast\n"
            "- **Infrastructure:** Specialized hardware (LPU)\n"
            "- **Best For:** Real-time applications, demos\n"
            "- **Pricing:** Very cost-effective")

st.divider()

# Google Gemini Models Section
st.header("ğŸ¤– Google Gemini Models (Multimodal AI)")

col5, col6 = st.columns(2)

with col5:
    st.subheader("âœ¨ gemini-2.0-flash")
    
    st.markdown("""
    **Provider:** Google  
    **Generation:** Latest (2.0)  
    **Speed:** âš¡âš¡ Medium (5-10 seconds)
    
    ### âœ… Best For:
    - Balanced performance
    - Real-time interactions
    - Multimodal understanding
    - Cost-effective solutions
    
    ### ğŸ’ª Strengths:
    - Latest Google AI technology
    - Good balance of speed/quality
    - Handles complex queries well
    - Free tier available
    
    ### âš ï¸ Limitations:
    - âš ï¸ Slower than Groq models
    - Response time varies with load
    - May have API rate limits
    
    ### ğŸ¯ Recommended Use Cases:
    ```
    âœ“ "Summarize this research paper"
    âœ“ "Compare different approaches to..."
    âœ“ "Create a detailed analysis of..."
    âœ“ Single Agent mode (faster)
    âœ“ When Groq is unavailable
    ```
    
    ### ğŸ’¡ Pro Tip:
    Good **alternative to Groq** when you need Google's reasoning style.
    """)

with col6:
    st.subheader("ğŸ§  gemini-2.5-pro")
    
    st.markdown("""
    **Provider:** Google  
    **Generation:** Advanced (2.5)  
    **Speed:** âš¡ Slower (8-15 seconds)
    
    ### âœ… Best For:
    - Complex reasoning tasks
    - Academic research
    - Professional analysis
    - High-quality content
    
    ### ğŸ’ª Strengths:
    - Most advanced reasoning
    - Superior analysis quality
    - Excellent for research
    - Handles nuanced queries
    
    ### âš ï¸ Limitations:
    - âš ï¸ **Slowest model** (8-15s single, 30-60s multi-agent)
    - Higher API costs
    - May hit rate limits faster
    - Not ideal for real-time use
    
    ### ğŸ¯ Recommended Use Cases:
    ```
    âœ“ "Conduct thorough analysis of..."
    âœ“ "Research the implications of..."
    âœ“ Academic papers and research
    âœ“ When quality > speed
    âœ— Real-time demos (too slow)
    âœ— Simple questions (overkill)
    ```
    
    ### ğŸ’¡ Pro Tip:
    Use for **final analysis** or when **quality is paramount**. Not for demos!
    """)

st.divider()

st.info("**â±ï¸ Gemini Performance Note:**\n\n"
        "Gemini models are slower than Groq, especially in Multi-Agent mode:\n"
        "- **Single Agent:** 5-15 seconds\n"
        "- **Multi-Agent Sequential:** 20-45 seconds\n"
        "- **Multi-Agent Debate:** 30-60 seconds\n\n"
        "ğŸ’¡ Tip: Use Groq for demos and presentations!")

st.divider()

# Quick Decision Guide
st.header("ğŸ¯ Quick Decision Guide")

decision_col1, decision_col2, decision_col3 = st.columns(3)

with decision_col1:
    st.markdown("""
    ### ğŸƒ Need Speed?
    
    **Choose:**
    - âš¡ `groq/compound-mini` (fastest)
    - âš¡ `llama-3.3-70b-versatile` (fast + quality)
    
    **Avoid:**
    - âŒ `gemini-2.5-pro` (slow)
    """)

with decision_col2:
    st.markdown("""
    ### ğŸ“ Need Quality?
    
    **Choose:**
    - ğŸ§  `openai/gpt-oss-120b` (best reasoning)
    - ğŸ§  `gemini-2.5-pro` (deep analysis)
    
    **Avoid:**
    - âŒ `compound-mini` (basic only)
    """)

with decision_col3:
    st.markdown("""
    ### âš–ï¸ Need Balance?
    
    **Choose:**
    - ğŸ¦™ `llama-3.3-70b-versatile` (all-rounder)
    - âœ¨ `gemini-2.0-flash` (good balance)
    
    **Best for:**
    - âœ… Most use cases
    """)

st.divider()

# Mode-Specific Recommendations
st.header("ğŸ”§ Mode-Specific Recommendations")

tab1, tab2, tab3 = st.tabs(["Single Agent", "Sequential Multi-Agent", "Debate Mode"])

with tab1:
    st.markdown("""
    ## Single Agent Mode
    
    ### âš¡ Fastest Setup:
    - **Model:** `groq/compound-mini`
    - **Web Search:** Disabled
    - **Use For:** Simple questions, definitions
    
    ### ğŸ¯ Best General Purpose:
    - **Model:** `llama-3.3-70b-versatile`
    - **Web Search:** Enabled
    - **Use For:** Most questions, code help, explanations
    
    ### ğŸ§  Best Quality:
    - **Model:** `openai/gpt-oss-120b`
    - **Web Search:** Enabled
    - **Use For:** Professional work, analysis
    """)

with tab2:
    st.markdown("""
    ## Sequential Multi-Agent Mode
    
    ### âš¡ Recommended (Fast):
    - **Model:** `llama-3.3-70b-versatile`
    - **Web Search:** Enabled
    - **Time:** ~10-15 seconds
    - **Use For:** Research questions, explanations
    
    ### ğŸ§  Advanced (Slower):
    - **Model:** `openai/gpt-oss-120b`
    - **Web Search:** Enabled
    - **Time:** ~15-25 seconds
    - **Use For:** Deep research, academic work
    
    ### âŒ Not Recommended:
    - `compound-mini` (no web search support)
    - `gemini-2.5-pro` (too slow: 30-45s)
    """)

with tab3:
    st.markdown("""
    ## Debate Multi-Agent Mode
    
    ### âš¡ Recommended (Fast):
    - **Model:** `llama-3.3-70b-versatile`
    - **Web Search:** Enabled
    - **Time:** ~15-20 seconds
    - **Use For:** Controversial questions, decisions
    
    ### ğŸ§  Best Quality (Slower):
    - **Model:** `openai/gpt-oss-120b`
    - **Web Search:** Enabled
    - **Time:** ~20-30 seconds
    - **Use For:** Important decisions, critical analysis
    
    ### âŒ Not Recommended:
    - `compound-mini` (no web search)
    - `gemini-2.5-pro` (very slow: 30-60s)
    
    ### ğŸ’¡ Perfect Questions for Debate:
    ```
    "Should I learn Python or JavaScript?"
    "Is remote work better than office?"
    "Should students use AI for homework?"
    "Is cryptocurrency a good investment?"
    ```
    """)

st.divider()

# Comparison Table
st.header("ğŸ“Š Model Comparison Table")

comparison_data = {
    "Model": [
        "llama-3.3-70b-versatile",
        "groq/compound-mini",
        "openai/gpt-oss-120b",
        "gemini-2.0-flash",
        "gemini-2.5-pro"
    ],
    "Provider": ["Groq", "Groq", "Groq", "Google", "Google"],
    "Speed": ["âš¡âš¡âš¡", "âš¡âš¡âš¡âš¡", "âš¡âš¡", "âš¡âš¡", "âš¡"],
    "Quality": ["â­â­â­â­", "â­â­", "â­â­â­â­â­", "â­â­â­â­", "â­â­â­â­â­"],
    "Web Search": ["âœ…", "âŒ", "âœ…", "âœ…", "âœ…"],
    "Multi-Agent": ["âœ… Recommended", "âŒ Not Supported", "âœ… Best", "âš ï¸ Slow", "âŒ Too Slow"],
    "Best For": ["General use", "Speed", "Quality", "Balance", "Research"]
}

st.table(comparison_data)

st.divider()

# Footer with tips
st.success("""
### ğŸ¯ Final Recommendations:

**For Demos/Presentations:**  
Use `llama-3.3-70b-versatile` or `groq/compound-mini` (Groq models are fast!)

**For Academic Work:**  
Use `openai/gpt-oss-120b` or `gemini-2.5-pro` (higher quality)

**For Real-Time Chat:**  
Use any Groq model with Single Agent mode

**For Important Decisions:**  
Use Debate Mode with `openai/gpt-oss-120b` or `llama-3.3-70b-versatile`

**When Unsure:**  
Start with `llama-3.3-70b-versatile` - it's the best all-rounder! ğŸ¦™
""")

st.divider()

# Back button
if st.button("â¬…ï¸ Back to Main App", type="primary", use_container_width=True):
    st.switch_page("D:/Mini Project/AI Agent/AI-agent/frontend.py")